---
permalink: /
title: "张宸，助理教授、博导，上海交通大学"
author_profile: true
redirect_from: 
  - /about/
  - /about.html
---

## 个人简介

> 我于2023年5月加入上海交通大学电子信息与电气工程学院，任长聘教轨助理教授，博士生导师，上海市海外高层次人才计划。主要研究方向是AI处理器架构和芯片系统。此前，于2017年从北京大学博士毕业，导师[丛京生教授](https://vast.cs.ucla.edu/people/faculty/jason-cong)和[孙广宇教授](https://ic.pku.edu.cn/szdw/zzjs/sjzdhyjsxtx1/sgy/index.htm)。期间于2015-2016前往美国加州大学洛杉矶分校([UCLA]((https://vast.cs.ucla.edu/people/alumni/chen-zhang)))学术访问。毕业后（2017-2023），任职微软研究院（主管研究员）和阿里巴巴平头哥半导体（架构师），参与并主导了多个国内外重要AI处理器与系统的研发工作。在 ISCA、MICRO、FPGA、DAC、T-CAD等国际会议和期刊发表论文30余篇，其中CCF(中国计算机学会)A类论文16篇，中美发明专利8项。谷歌学术统计，第一作者/通讯作者论文总引用4100余次，近5年总引用3400余次。


## Biography

> I joined the Department of Micro-Nano Electronics at the School of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University as a tenure-track Assistant Professor in May 2023. Prior to this, I obtained my Ph.D. degree from Peking University in 2017, advised by [Prof. Jason Cong](https://vast.cs.ucla.edu/people/faculty/jason-cong)and [Prof. Guangyu Sun](https://ic.pku.edu.cn/szdw/zzjs/sjzdhyjsxtx1/sgy/index.htm), specializing in research on artificial intelligence accelerators. During my doctoral studies, I conducted academic visits at the University of California, Los Angeles (UCLA) in 2015-2016 [Alumni page](https://vast.cs.ucla.edu/people/alumni/chen-zhang). After graduation, I joined Microsoft Research, where I worked on cloud-edge-end artificial intelligence systems and heterogeneous acceleration. I progressed to the position of Senior Researcher. In early 2021, I joined Alibaba T-head Semiconductor Co., Ltd., leading the parallel computing team as a Core Chip Architect, responsible for artificial intelligence algorithm acceleration and cluster chip interconnect-related functional design.

Selected Publications（[Full List](https://chenzhangsjtu.github.io/publications/)）
======
- H^2-LLM: Hardware-Dataflow Co-Exploration for Heterogeneous Hybrid-Bonding-based Low-Batch LLM Inference, **ISCA**, 2025
- SynGPU: Synergizing CUDA and Bit-Serial Tensor Cores for Vision Transformer Acceleration on GPU, **DAC**, 2025
- Oltron: Software-Hardware Co-design for Outlier-Aware Quantization of LLMs with Inter-/Intra-Layer Adaptation, **DAC**, 2024
- Dual-side sparse tensor core, **ISCA**, 2021
- Caffeine: Toward uniformed representation and acceleration for deep convolutional neural networks, **T-CAD**, 2018
- Optimizing FPGA-based accelerator design for deep convolutional neural networks, **FPGA**, 2015


Awards and Honors
======
- [2025] FPGA and Reconfigurable Computing Hall of Fame (名人堂)
- [2024] Stanford and Elsevier Top-2% Most Cited Scholars (Computer Architecture and Hardware)
- [2021~2024] AI-2000 World's Most Influential Scholars (AI Chip)
- [2023] MICRO Top Picks (Honorable Mention)
- [2022] ACM ChinaSys Rising Star
- [2019] Donald O. Pederson Best Paper
- [2019] Microsoft Research Special Stock Award
- [2015] FPGA Best Paper Nomination